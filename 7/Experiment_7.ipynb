{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92c1b7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9HCLKkbHzrt",
    "outputId": "b7efdf13-aaba-4354-82ba-a2262144c0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f762b48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lezWe04nHzr0",
    "outputId": "8a2c4a73-2882-4437-c8d3-b0ea0239cc24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ddca29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwpx3I0iHzr1",
    "outputId": "f10352e5-9286-4703-8b92-63ac40578db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Welcome to Smallpdf\n",
      "Digital Documents—All In One Place\n",
      "Access Files Anytime, Anywhere Enhance Documents in One Click \n",
      "Collaborate With Others With the new Smallpdf experience, you can \n",
      "freely upload, organize, and share digital \n",
      "documents. When you enable the ‘Storage’ \n",
      "option, we’ll also store all processed files here. \n",
      "You can access files stored on Smallpdf from \n",
      "your computer, phone, or tablet. We’ll also \n",
      "sync files from the Smallpdf Mobile App to our \n",
      "online portalWhen you right-click on a file, we’ll present \n",
      "you with an array of options to convert, \n",
      "compress, or modify it. \n",
      "Forget mundane administrative tasks. With \n",
      "Smallpdf, you can request e-signatures, send \n",
      "large files, or even enable the Smallpdf G Suite \n",
      "App for your entire organization. Ready to take document management to the next level? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file\n",
    "pdfFileObj = open(r\"sample.pdf\", 'rb')\n",
    "\n",
    "# Create a PDF reader object\n",
    "pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "\n",
    "# Print number of pages\n",
    "print(len(pdfReader.pages))\n",
    "\n",
    "# Get the first page\n",
    "pageObj = pdfReader.pages[0]\n",
    "\n",
    "# Extract text from page\n",
    "print(pageObj.extract_text())\n",
    "\n",
    "# Close the PDF file\n",
    "pdfFileObj.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd47f10a",
   "metadata": {
    "id": "9OqmTpY4Hzr3"
   },
   "outputs": [],
   "source": [
    "# import docx NOT python-docx\n",
    "import docx\n",
    "  \n",
    "# create an instance of a word document\n",
    "doc = docx.Document()\n",
    "  \n",
    "# add a heading of level 0 (largest heading)\n",
    "doc.add_heading('Heading for the document', 0)\n",
    "  \n",
    "# add a paragraph and store \n",
    "# the object in a variable\n",
    "doc_para = doc.add_paragraph('Your paragraph goes here, ')\n",
    "  \n",
    "# add a run i.e, style like \n",
    "# bold, italic, underline, etc.\n",
    "doc_para.add_run('hey there, bold here').bold = True\n",
    "doc_para.add_run(', and ')\n",
    "doc_para.add_run('these words are italic').italic = True\n",
    "  \n",
    "# add a page break to start a new page\n",
    "doc.add_page_break()\n",
    "  \n",
    "# add a heading of level 2\n",
    "doc.add_heading('Heading level 2', 2)\n",
    "  \n",
    "# pictures can also be added to our word document\n",
    "# width is optional\n",
    "doc.add_picture(r\"index.jpg\")\n",
    "  \n",
    "# now save the document to a location\n",
    "doc.save('new_doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8cf2aa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVbi3BHwHzr4",
    "outputId": "0028cd1a-6594-49b1-e8d0-73a5b3d6aadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19935524",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdnoGOs1Hzr5",
    "outputId": "1681934c-db0a-4783-83ae-a0ff4b1cdfa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c7cbff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdJULL3IHzr6",
    "outputId": "2ed680be-a0ad-4f5f-9ca5-4c991216ac4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The First sentence is about Python.', 'The Second: about Django.', 'You can learn Python,Django and Data Ananlysis here.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "\n",
    "sentence_data = \"The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here. \"\n",
    "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53e1a23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFSoVWabHzr7",
    "outputId": "45e190f9-fa6a-4c5f-a421-4cde12e7312d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wie geht es Ihnen?', 'Gut, danke.']\n"
     ]
    }
   ],
   "source": [
    "#Non English language Tokenization\n",
    "\n",
    "german_tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "german_tokens=german_tokenizer.tokenize('Wie geht es Ihnen?  Gut, danke.')\n",
    "print(german_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901c4ba4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwNd_fYoHzr8",
    "outputId": "8aa8f23a-2b44-4d95-962d-d9a98741fbbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'originated', 'from', 'the', 'idea', 'that', 'there', 'are', 'readers', 'who', 'prefer', 'learning', 'new', 'skills', 'from', 'the', 'comforts', 'of', 'their', 'drawing', 'rooms']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5970b443",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5NXxeG4Hzr9",
    "outputId": "21e90076-8ec2-474b-f1ac-3aa119f89b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'is', 'a', 'boy', '.', 'She', 'is', 'a', 'girl']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "  \n",
    "#Dummy text\n",
    "txt = \"He is a boy. \"\\\n",
    "    \"She is a girl\"\n",
    "\n",
    "word_tokens = word_tokenize(txt)\n",
    "  \n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1726ea5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1q_7aeTHzr-",
    "outputId": "53050fa9-b159-4b78-a8a9-472bcd8f5775"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('welcome', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('learn', 'VB'),\n",
       " ('Categorizing', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('POS', 'NNP'),\n",
       " ('Tagging', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Python', 'NNP')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part of Speech (POS) tagging\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd608de5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7JOBlU3Hzr_",
    "outputId": "1a37d5d5-4ac8-420a-de88-f3083bf76711"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1633dbb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdRww8lTHzsA",
    "outputId": "97798dff-47f3-44b6-8c29-ac76391697b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcb0bbdd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-v8o9aRHzsA",
    "outputId": "b12abe0d-dc63-455d-e834-62c33b52efaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "Stop Words Removed: ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stopwords removal from sentence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(\"Tokenized:\", word_tokens)\n",
    "print(\"Stop Words Removed:\", filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9771c078",
   "metadata": {
    "id": "wbAKWeX0HzsC"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\College\\\\TE\\\\SEM-2\\\\Practical\\\\DSBDA\\\\7\\\\text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# word_tokenize accepts\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# a string as an input, not a file.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 10\u001b[0m file1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCollege\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSEM-2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPractical\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDSBDA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m7\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtext.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Use this to read file content as a stream:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m line \u001b[38;5;241m=\u001b[39m file1\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\College\\\\TE\\\\SEM-2\\\\Practical\\\\DSBDA\\\\7\\\\text.txt'"
     ]
    }
   ],
   "source": [
    "#Stopwords from input file\n",
    "\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(r\"D:\\College\\TE\\SEM-2\\Practical\\DSBDA\\7\\text.txt\")\n",
    " \n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6af0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooyFq1aJHzsC",
    "outputId": "7b8b00f0-40b9-4bc6-dd26-018792994a2d"
   },
   "outputs": [],
   "source": [
    "#Stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"It vijaying meeting better vijayed vijays eats skills originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "for w in nltk_tokens:\n",
    "       print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649adc5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSmXKl9MHzsD",
    "outputId": "bb510e80-fcbe-4748-887c-5bb4339cc916"
   },
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_data = \"It studies densely is better  meeting studying vijaying vijayed vijays skills originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "        print(\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85707ba",
   "metadata": {
    "id": "J8SNEOlSHzsD"
   },
   "outputs": [],
   "source": [
    "#Expt.No.7 2nd Operation\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05fb32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJyLNJNEHzsD",
    "outputId": "9554d79c-d3d0-4b15-cf6a-defd5ae308b5"
   },
   "outputs": [],
   "source": [
    "first_sentence = \"Data Science is the best job of the 21st century\"\n",
    "second_sentence = \"Machine learning is the key for data science\"\n",
    "\n",
    "#split so each word have their own string\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770b33c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "7_Op7_EHHzsE",
    "outputId": "65c157ae-3735-454d-fc95-ff6f41f7da48"
   },
   "outputs": [],
   "source": [
    "#count the words\n",
    "\n",
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf8a53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "MxENejj3HzsE",
    "outputId": "6d95205b-1b6c-4703-e51e-a6e6d604c326"
   },
   "outputs": [],
   "source": [
    "#Compute Term Frequency(TF)\n",
    "\n",
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "pd.DataFrame([tfFirst, tfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956b5cd",
   "metadata": {
    "id": "L6mbtNcrHzsF"
   },
   "outputs": [],
   "source": [
    "#Compute Inverse Document Frequency(IDF)\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b4128",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "QhZ1RQt3HzsF",
    "outputId": "2d1a71c7-21b3-4d73-f1fb-91b2874c92e3"
   },
   "outputs": [],
   "source": [
    "#Compute Term Frequency(TF) - Inverse Document Frequency(IDF)\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "\n",
    "#putting it in a dataframe\n",
    "pd.DataFrame([idfFirst, idfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3b491",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-6YcxsJHzsG",
    "outputId": "8c51d011-4a07-4231-df8c-35bf1a90ca97"
   },
   "outputs": [],
   "source": [
    "#Compute TF-IDF\n",
    "\n",
    "#first step is to import the library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#for the sentence, make sure all words are lowercase or you will run #into error. for simplicity, I just made the same sentence all #lowercase\n",
    "firstV= \"Data Science is the sexiest job of the 21st century\"\n",
    "secondV= \"machine learning is the key for data science\"\n",
    "\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([firstV, secondV])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa58d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Expt.No.7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
